{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters  import TokenTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TokenTextSplitter in LangChain is a text splitter that breaks down large chunks of text into smaller pieces based \n",
    "on token count rather than character count. This is useful for handling large documents efficiently while ensuring they fit within the token limits of LLMs.\n",
    "\n",
    "Usage and Purpose\n",
    "The TokenTextSplitter is primarily used to:\n",
    "\n",
    "Ensure token limit compliance – LLMs like OpenAI's GPT models have token limits, so splitting text into manageable chunks prevents exceeding those limits.\n",
    "Improve retrieval quality – Splitting text into meaningful chunks allows better retrieval in RAG (Retrieval-Augmented Generation) systems.\n",
    "Enhance context understanding – By breaking large documents into semantically meaningful pieces, the model processes them more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* is used to unpack a list of strings extracted from page_content attributes of Document objects.\n",
    "sep=\"\\n\" ensures each document content is printed on a new line.\n",
    "This technique is useful when quickly displaying or processing multiple documents efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain simplifies building applications with LLMs\n",
      " with LLMs by providing abstractions for chains,\n",
      " for chains, memory, and agents.\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sample text\n",
    "text = \"LangChain simplifies building applications with LLMs by providing abstractions for chains, memory, and agents.\"\n",
    "\n",
    "# Initialize TokenTextSplitter\n",
    "splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=3)\n",
    "\n",
    "# Split text into token-based chunks\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Using * operator to print chunks\n",
    "print(*chunks, sep=\"\\n\")\n",
    "print(len(chunks))\n",
    "type(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain enables LLM-powered applications.\n",
      "Tokenization is essential for handling large texts.\n",
      "Vector databases store embeddings for efficient search.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Creating multiple Document objects\n",
    "doc1 = Document(page_content=\"LangChain enables LLM-powered applications.\", metadata={\"source\": \"AI Guide\"})\n",
    "doc2 = Document(page_content=\"Tokenization is essential for handling large texts.\", metadata={\"source\": \"NLP Handbook\"})\n",
    "doc3 = Document(page_content=\"Vector databases store embeddings for efficient search.\", metadata={\"source\": \"DB Essentials\"})\n",
    "\n",
    "# Storing them in a list\n",
    "documents = [doc1, doc2, doc3]\n",
    "\n",
    "# Using * to unpack and print each document's content\n",
    "print(*[doc.page_content for doc in documents], sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Document class from LangChain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Define raw text content\n",
    "raw_text = \"The xAI Grok 3 is an advanced AI model designed to assist users in various tasks.\"\n",
    "\n",
    "# Create a Document object with page_content and optional metadata\n",
    "doc = Document(\n",
    "    page_content=raw_text,\n",
    "    metadata={\n",
    "        \"source\": \"xAI Product Catalog\",\n",
    "        \"title\": \"Grok 3 Overview\",\n",
    "        \"date\": \"March 04, 2025\"\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The xAI Grok 3 is an advanced AI model designed to assist users in various tasks.' metadata={'source': 'xAI Product Catalog', 'title': 'Grok 3 Overview', 'date': 'March 04, 2025'}\n"
     ]
    }
   ],
   "source": [
    "# Print the document to verify\n",
    "print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
